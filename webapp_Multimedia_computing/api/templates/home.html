{% extends 'base.html' %}

{% block content %}

<section id="first" class="main">
      <header>
        <div class="container">
          <h2><b>PROJECT MULTIMEDIA COMPUTING</b></h2>
          <h3>Implementation of Compression and Decompression</h3>

          <h4><b>Members:</b><br/>
            Tran Quoc Long - 14520490<br />
            Hoang Ngoc Thach - <br />
            Le Thi Ngoc Thuy - <br />
          </h4>
          <p>This project includes: Huffman, Arithmetic, LZW </p>
        </div>
      </header>
        <div class="content dark style3 featured">

        <div><p><font size="15"><h1>Selections<br><br></h1></font></p></div>
        <div class="row">
          <div class="12u">
            <footer>
              <a href="{% url 'compression' %}" class="button">Compression</a>
              <a href="{% url 'decompression' %}" class="button">Decompression</a>
            </footer>
          </div>
        </div>
        </div>
        <div class="content dark style1 featured">
        

          <div><p><font size="15"><h1>Some informations<br><br></h1></font></p></div>

        <div class="container">
          <div class="row">
            <div class="4u 12u(narrow)">
              <section>
                <span class="feature-icon"><span class="icon fa-clock-o"></span></span>
                <header>
                  <h3>Huffman</h3>
                </header>
                <p>In computer science and information theory, a <b>Huffman</b> code is a particular type of optimal <i>prefix</i> code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes". <br />

                The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.</p>
              </section>
            </div>
            <div class="4u 12u(narrow)">
              <section>
                <span class="feature-icon"><span class="icon fa-bolt"></span></span>
                <header>
                  <h3>Arithmetic</h3>
                </header>
                <p> Arithmetic coding is a form of entropy encoding used in lossless data compression. Normally, a string of characters such as the words "hello there" is represented using a fixed number of bits per character, as in the ASCII code. When a string is converted to arithmetic encoding, frequently used characters will be stored with fewer bits and not-so-frequently occurring characters will be stored with more bits, resulting in fewer bits used in total. Arithmetic coding differs from other forms of entropy encoding, such as Huffman coding, in that rather than separating the input into component symbols and replacing each with a code, arithmetic coding encodes the entire message into a single number, an arbitrary-precision fraction q where 0.0 ≤ q < 1.0. It represents the current information as a range, defined by two numbers. Recent Asymmetric Numeral Systems family of entropy coders allows for faster implementations thanks to directly operating on a single natural number representing the current information.</p>
              </section>
            </div>
            <div class="4u 12u(narrow)">
              <section>
                <span class="feature-icon"><span class="icon fa-cloud"></span></span>
                <header>
                  <h3>LZW</h3>
                </header>
                <p><b>Lempel–Ziv–Welch (LZW)</b> is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as <i>an improved implementation of the LZ78 algorithm</i> published by Lempel and Ziv in 1978.<br> - The algorithm is simple to implement and has the potential for very high throughput in hardware implementations.[1] It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format.</p>
              </section>
            </div>
          </div>

        </div>
      </div>
</section>
{% endblock %}